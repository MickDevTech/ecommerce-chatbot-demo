# Upgrade a Mistral-7B-Instruct-v0.2

## ‚úÖ Cambios Realizados en el C√≥digo

Se ha optimizado el c√≥digo para usar **Mistral-7B-Instruct-v0.2**, un modelo mucho m√°s capaz que TinyLlama:

### Comparaci√≥n de Modelos

| Caracter√≠stica | TinyLlama-1.1B | Mistral-7B |
|----------------|----------------|------------|
| **Par√°metros** | 1.1B | 7B (6.4x m√°s grande) |
| **Razonamiento** | Limitado | Excelente |
| **Coherencia** | Baja | Alta |
| **Seguimiento de instrucciones** | Regular | Excelente |
| **Respuestas largas** | Dif√≠cil | Natural |
| **Velocidad (CPU)** | ~20-30 seg | ~40-60 seg |
| **RAM requerida** | ~2GB | ~8-10GB |

---

## üìù Paso 1: Actualizar `.env`

Abre tu archivo `.env` y cambia estas l√≠neas:

```bash
# CAMBIAR ESTAS L√çNEAS:
HF_MODEL_ID=mistralai/Mistral-7B-Instruct-v0.2
USE_LOCAL_MODEL=true
USE_8BIT_QUANTIZATION=true  # Reduce uso de memoria de ~14GB a ~7GB
```

**IMPORTANTE:** La cuantizaci√≥n 8-bit es NECESARIA para ejecutar Mistral-7B en m√°quinas con menos de 16GB de RAM.

**Archivo completo `.env` deber√≠a verse as√≠:**

```bash
HF_TOKEN=tu_token_de_hugging_face
HF_MODEL_ID=mistralai/Mistral-7B-Instruct-v0.2
USE_LOCAL_MODEL=true
ALLOWED_ORIGINS=http://localhost:5173,http://127.0.0.1:5173,http://localhost:3000,http://127.0.0.1:3000,*
VITE_API_URL=http://localhost:8000
```

---

## üöÄ Paso 2: Reiniciar el Backend

```bash
docker compose down
docker compose up -d
```

**Primera carga:** El modelo se descargar√° (~4GB) y puede tardar 2-5 minutos.

---

## üéØ Mejoras Implementadas

### 1. **Prompts Optimizados para Mistral**

Ahora usa el formato oficial de Mistral:
```
<s>[INST] Instrucciones... [/INST]
```

### 2. **Par√°metros Ajustados**

```python
# Productos espec√≠ficos/detalles:
max_tokens = 400      # M√°s espacio (antes: 300)
temperature = 0.7     # M√°s creativo (antes: 0.4)

# Consultas generales:
max_tokens = 300      # M√°s espacio (antes: 200)
temperature = 0.8     # M√°s natural (antes: 0.6)

# Otros:
repetition_penalty = 1.1  # Menos restrictivo (antes: 1.3)
```

### 3. **Limpieza de Respuestas Mejorada**

Maneja correctamente el formato de salida de Mistral:
```
<s>[INST]...[/INST] RESPUESTA_AQU√ç</s>
```

---

## üìä Resultados Esperados

### Antes (TinyLlama):
```
Usuario: "Dame informaci√≥n sobre Laptop 14"
Bot: "Laptop 14 price $799.99"  ‚ùå (corto, poco natural)
```

### Ahora (Mistral-7B):
```
Usuario: "Dame informaci√≥n sobre Laptop 14"
Bot: "¬°Claro! Aqu√≠ est√° toda la informaci√≥n sobre la Laptop 14":

‚Ä¢ Laptop 14"

‚Ä¢ Precio: $799.99

‚Ä¢ Categor√≠a: electr√≥nica

‚Ä¢ Stock disponible: 8 unidades

‚Ä¢ Descripci√≥n: Port√°til 14 pulgadas, 16GB RAM, 512GB SSD.

¬øTe gustar√≠a saber algo m√°s sobre este producto?"
```
‚úÖ (completo, natural, bien formateado)

---

## ‚ö†Ô∏è Consideraciones

### Requisitos de Hardware

**Con cuantizaci√≥n 8-bit (USE_8BIT_QUANTIZATION=true):**
- **RAM m√≠nima:** 7-8GB
- **Espacio en disco:** ~5GB para el modelo
- **CPU:** Cualquier procesador moderno (m√°s cores = m√°s r√°pido)

**Sin cuantizaci√≥n (USE_8BIT_QUANTIZATION=false):**
- **RAM m√≠nima:** 14-16GB
- **Mejor calidad** pero mucho m√°s lento

### Tiempos de Respuesta

- **Primera consulta:** ~2-3 minutos (carga del modelo)
- **Consultas siguientes:** ~40-60 segundos
- **Con GPU:** ~5-10 segundos (si tienes NVIDIA GPU)

### Alternativas si es muy lento

Si Mistral-7B es muy lento en tu m√°quina, puedes usar:

1. **Mistral-7B v√≠a API** (m√°s r√°pido, pero con cuotas):
   ```bash
   USE_LOCAL_MODEL=false
   ```

2. **Modelo m√°s peque√±o** (menos capaz pero m√°s r√°pido):
   ```bash
   HF_MODEL_ID=google/flan-t5-large
   ```

---

## üß™ Pruebas Sugeridas

Despu√©s de reiniciar, prueba estas consultas:

1. **Producto espec√≠fico:**
   - "Dame informaci√≥n sobre Laptop 14"
   - "¬øQu√© stock hay de Auriculares Inal√°mbricos?"

2. **Categor√≠a:**
   - "¬øTienes camisetas?"
   - "Mu√©strame productos de electr√≥nica"

3. **Consulta compleja:**
   - "Recomi√©ndame algo para trabajar desde casa"
   - "¬øQu√© productos tienes entre $50 y $150?"

---

## üìà Monitoreo

Observa los logs para ver el progreso:

```bash
docker compose logs -f backend
```

Deber√≠as ver:
```
[INFO] generando respuesta con modelo Mistral-7B-Instruct-v0.2...
[INFO] respuesta del modelo: ¬°Claro! Aqu√≠ est√° toda la informaci√≥n...
```

---

## üêõ Troubleshooting

### Error: "Out of memory"
- Reduce `max_tokens` a 200-300
- Cierra otras aplicaciones
- Considera usar la API en lugar de local

### Error: "Model download failed"
- Verifica tu conexi√≥n a internet
- Verifica que tu `HF_TOKEN` sea v√°lido
- Intenta descargar manualmente: `huggingface-cli download mistralai/Mistral-7B-Instruct-v0.2`

### Respuestas en ingl√©s
- Mistral maneja mucho mejor el espa√±ol que TinyLlama
- Si persiste, verifica que el prompt incluya "Responde en espa√±ol"

---

## ‚ú® Beneficios del Upgrade

1. ‚úÖ **Respuestas m√°s largas y completas**
2. ‚úÖ **Mejor seguimiento de instrucciones**
3. ‚úÖ **Formato m√°s consistente**
4. ‚úÖ **Razonamiento mejorado**
5. ‚úÖ **Menos respuestas en ingl√©s**
6. ‚úÖ **Tono m√°s natural y amable**

---

¬°Disfruta de tu chatbot mejorado! üöÄ
