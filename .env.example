# Copia este archivo a .env y completa tu token de Hugging Face
HF_TOKEN=tu_token_de_hugging_face

# Modelo de Hugging Face
# IMPORTANTE: usa modelos de la API gratuita, NO del router pagado (:fastest, :free, etc)
# Modelos recomendados gratuitos:
# - mistralai/Mistral-7B-Instruct-v0.2 (recomendado)
# - meta-llama/Llama-2-7b-chat-hf (requiere aceptar licencia en HF)
# - google/flan-t5-large
HF_MODEL_ID=mistralai/Mistral-7B-Instruct-v0.2

# Ejecutar modelo localmente (sin cuotas de API)
# true = usa transformers localmente (sin límites, pero requiere RAM y es más lento)
# false = usa API de Hugging Face (rápido, pero tiene cuotas mensuales)
# Para modelos pequeños (<1GB) en CPU, considera: google/flan-t5-base, distilgpt2
USE_LOCAL_MODEL=false

# Backend: CORS - Orígenes permitidos (separados por coma)
# Ejemplo: http://localhost:5173,https://tu-app.netlify.app
ALLOWED_ORIGINS=http://localhost:5173,http://127.0.0.1:5173

# Frontend: URL del API backend
# En desarrollo local: http://localhost:8000
# En producción: tu URL de backend desplegado
VITE_API_URL=http://localhost:8000
